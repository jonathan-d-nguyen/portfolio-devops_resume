name: resume
# extra notes at bottom

services:
  jenkins:
    build:
      context: .
      dockerfile: jenkins.Dockerfile
    volumes:
      - $PWD/jenkins_home:/var/jenkins_home
      - $PWD:/app # if using git, wouldn't need this second volume line b/c project in git ;;
      # just clone from git
    ports:
      - 8080:8080
  aws:
    image: amazon/aws-cli
    # entrypoint: bash
    # running this with the entrypoint: bash with the given
    # volumes allows you to start at root within the container
    # docker-compose run --rm aws
    # environment:
    #   AWS_REGION: "us-east-1"
    volumes: #local path : container's path
      - $PWD:/app
      - /Users/jonathannguyen/.aws/credentials:/root/.aws/credentials
      - /Users/jonathannguyen/.aws/:/root/.aws/
    working_dir: /app #if using bash entrypoint, you will start here once you run this aws service container

    # to transfer files to s3:
    # with entrypoint:
    # docker-compose run --rm aws s3 cp --recursive /app/website s3://www.jdnguyen.tech
    # without entrypoint:
    # docker-compose run --rm --entrypoint aws aws s3 cp --recursive /app/website s3://jdnguyen.tech
    # aws s3 cp --recursive /app/website/ s3://explorecalifornia.org/

    # /app/website breakdown: in this command, you are copying from the aws container to the s3 bucket ;; path is relative to container:
    # working directory of
  terraform:
    # entrypoint: /bin/sh
    build:
      dockerfile: terraform.Dockerfile
      context: .
      # making this available in docker compose as a service
    environment:
      AWS_REGION: "us-east-1"
    volumes:
      # - $PWD:/app
      # - /Users/jonathannguyen/.aws/credentials:/app/.aws/credentials
      # - /Users/jonathannguyen/.aws/config:/app/.aws/config
      - .:/app # Mount project root to /app
      - $HOME/.aws/credentials:/home/tfuser/.aws/credentials:ro # AWS credentials
      - $HOME/.aws/config:/home/tfuser/.aws/config:ro # AWS config
    working_dir: /app # Where commands will be executed
  selenium:
    image: selenium/standalone-chrome
    platform: linux/x86_64
    ports:
      - 4444:4444
      - 5901:5900
  website:
    build:
      context: .
    ports:
      - 80:80
  integration-tests:
    build:
      dockerfile: rspec.dockerfile
      context: .
    environment:
      SELENIUM_HOST: selenium
      SELENIUM_PORT: 4444
      WEBSITE_URL: www.jdnguyen.tech.s3-website-us-east-1.amazonaws.com
    volumes:
      - $PWD:/app
    entrypoint: rspec
    command:
      - --pattern
      - /app/spec/integration/*_spec.rb
  unit-tests:
    build:
      dockerfile: rspec.dockerfile
      context: .
    environment:
      SELENIUM_HOST: selenium
      SELENIUM_PORT: 4444
    volumes:
      - $PWD:/app
    entrypoint: rspec
    command:
      - --pattern
      - /app/spec/unit/*_spec.rb
# docker-compose run --rm aws sts get-caller-identity
# {
#     "UserId": "AIDAXYKJW4HUBWV45DE6V",
#     "Account": "533267407336",
#     "Arn": "arn:aws:iam::533267407336:user/iamadmin-gen"
# }

# Tokugero â€” Yesterday at 8:15 PM
# @_NoBadDays tldr:
#
# Docker host level mounting at it's simplest is taking a host level directory and putting it on top of a container level directory
#
# When you pass a command to the container, you need to reference paths relative to the container process, not to the host itself
#
# Putting that together, if you want to use a container to run command touching your host system files, you need to mount your host system files
#  to the container path and then reference that container path in your command
#
# A gotcha in your example as well: $PWD when accessed from your host is your hosts environment for the present working
#  directory, bash will substitute that environment value before the request gets to the container which violates the above parameters I mentioned
